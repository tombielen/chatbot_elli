# Elli: AI-Powered Chatbot for Mental Health Screening

**Author**: Tom Bielen  
**Affiliation**: IU International University of Applied Sciences  
**Preprint**: [PsyArXiv – The Illusion of Empathy (link pending)](https://psyarxiv.com/xxxxxx)  
**Preregistration**: [OSF Protocol](https://osf.io/6yrkw/)  
**License**: MIT

---

This repository contains **Elli**, a GPT-4-powered research chatbot developed for an independent, preregistered experimental study. Elli conducts conversational mental health screening using the PHQ-9 and GAD-7 with adaptive, empathic dialogue. The project compares this chatbot interface against a static web form to evaluate trust, emotional response, and user disclosure.

> ⚠️ **Disclaimer**:  
> Elli is a **research prototype only** and is not intended for diagnosis, therapy, or crisis support.  
> If you are in emotional distress, please seek help from a qualified mental health professional or hotline.

---

## 🧠 Study Overview

**Study Title**:  
_The Illusion of Empathy: Why Users Distrust GPT-4 Chatbots for Mental Health Screening_

**Objective**:  
Evaluate whether an AI chatbot offering simulated empathy enhances or hinders trust, comfort, and disclosure compared to a static mental health screening form.

---

## 🔬 Methodology

- **Design**: Randomized, cross-sectional, between-subjects, mixed-methods experiment
- **Participants**: 149 adults (age 18+), English-speaking, recruited online
- **Conditions**:
  - **Elli Chatbot**: Adaptive, GPT-4-powered, empathic
  - **Static Form**: No feedback, simple input form (control)
- **Measures**:
  - PHQ-9 and GAD-7 (validated tools)
  - Trust, comfort, and empathy (Likert ratings)
  - Completion time, dropout rate, and open-ended self-disclosure